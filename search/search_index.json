{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About # This mini-website is meant as both a short documentation regarding the things our team has done throughout our CS3244 machine learning project, as well as a guide to our GitHub repository that will serve as a copy of its README. Furthermore, this documentation contains the plots accompanying our model evaluations as well as short write-ups to explain said plots. Using the website # Navigate the website by searching (on the top right searchbar) or by clicking any text inside the left navbar. Each page's table of contents will be displayed on the right hand side of the screen. Toggle between light and dark mode by pressing the \"sun\" icon on the top right. CS3244 Team PG26 members # Lim Koon Kiat Ng Wen Jun Ryan Tan Wei Jie S. K. Haridharan Wilson Widyadhana Veronica Angelin Setiyo","title":"Home"},{"location":"#about","text":"This mini-website is meant as both a short documentation regarding the things our team has done throughout our CS3244 machine learning project, as well as a guide to our GitHub repository that will serve as a copy of its README. Furthermore, this documentation contains the plots accompanying our model evaluations as well as short write-ups to explain said plots.","title":"About"},{"location":"#using-the-website","text":"Navigate the website by searching (on the top right searchbar) or by clicking any text inside the left navbar. Each page's table of contents will be displayed on the right hand side of the screen. Toggle between light and dark mode by pressing the \"sun\" icon on the top right.","title":"Using the website"},{"location":"#cs3244-team-pg26-members","text":"Lim Koon Kiat Ng Wen Jun Ryan Tan Wei Jie S. K. Haridharan Wilson Widyadhana Veronica Angelin Setiyo","title":"CS3244 Team PG26 members"},{"location":"credits/","text":"Website # This website was built with MkDocs and Material for MkDocs . API references # In this project, we have used various libraries and their API documentations. These include, but are not limited to: Scikit-learn XGBoost TensorFlow and its high-level API Keras NumPy Scipy Plotly Express Seaborn Matplotlib Pandas Pandas Profiling Python Documentation Statsmodels OneMap API Google Maps API Geopandas Shapely GitHub Docs Online resources # Additionally, we used Google Colaboratory to quicken model training and/or evaluation, as well as these online resources for references and ideas on how to run our project: https://towardsdatascience.com/predicting-hdb-housing-prices-using-neural-networks-94ab708cccf8 https://towardsdatascience.com/predict-the-selling-price-of-hdb-resale-flats-50530391a845 https://royleekiat.com/2020/10/22/how-to-predict-hdb-resale-prices-using-3-different-machine-learning-models-linear-regression-neural-networks-k-nearest-neighbours/ https://www.kaggle.com/misterkix/prediction-of-singapore-hdb-price-machine-learning https://xiongkexin.github.io/assets/report3.pdf https://towardsdatascience.com/using-scikit-learns-binary-trees-to-efficiently-find-latitude-and-longitude-neighbors-909979bd929b https://stackoverflow.com/questions/70979065/how-to-plot-a-map-with-time-slider-and-zoom-on-a-city-with-plotly-in-python https://stackoverflow.com/questions/57383651/how-to-change-the-colorbar-range-in-plotly-express-graph https://stackoverflow.com/questions/42822768/pandas-number-of-months-between-two-dates https://stackoverflow.com/questions/15910019/annotate-data-points-while-plotting-from-pandas-dataframe Datasets # The datasets that we have used are listed below: HDB resale flat prices dataset MAS core inflation measure (CPI) dataset Singapore overnight rate average (SORA) data Special mentions # We would like to thank Prof. Kan Min-Yen and Prof. Brian Lim for their teaching in this module, as well as our project mentor Tian Xiao for the advice given to us when working through this project.","title":"Credits"},{"location":"credits/#website","text":"This website was built with MkDocs and Material for MkDocs .","title":"Website"},{"location":"credits/#api-references","text":"In this project, we have used various libraries and their API documentations. These include, but are not limited to: Scikit-learn XGBoost TensorFlow and its high-level API Keras NumPy Scipy Plotly Express Seaborn Matplotlib Pandas Pandas Profiling Python Documentation Statsmodels OneMap API Google Maps API Geopandas Shapely GitHub Docs","title":"API references"},{"location":"credits/#online-resources","text":"Additionally, we used Google Colaboratory to quicken model training and/or evaluation, as well as these online resources for references and ideas on how to run our project: https://towardsdatascience.com/predicting-hdb-housing-prices-using-neural-networks-94ab708cccf8 https://towardsdatascience.com/predict-the-selling-price-of-hdb-resale-flats-50530391a845 https://royleekiat.com/2020/10/22/how-to-predict-hdb-resale-prices-using-3-different-machine-learning-models-linear-regression-neural-networks-k-nearest-neighbours/ https://www.kaggle.com/misterkix/prediction-of-singapore-hdb-price-machine-learning https://xiongkexin.github.io/assets/report3.pdf https://towardsdatascience.com/using-scikit-learns-binary-trees-to-efficiently-find-latitude-and-longitude-neighbors-909979bd929b https://stackoverflow.com/questions/70979065/how-to-plot-a-map-with-time-slider-and-zoom-on-a-city-with-plotly-in-python https://stackoverflow.com/questions/57383651/how-to-change-the-colorbar-range-in-plotly-express-graph https://stackoverflow.com/questions/42822768/pandas-number-of-months-between-two-dates https://stackoverflow.com/questions/15910019/annotate-data-points-while-plotting-from-pandas-dataframe","title":"Online resources"},{"location":"credits/#datasets","text":"The datasets that we have used are listed below: HDB resale flat prices dataset MAS core inflation measure (CPI) dataset Singapore overnight rate average (SORA) data","title":"Datasets"},{"location":"credits/#special-mentions","text":"We would like to thank Prof. Kan Min-Yen and Prof. Brian Lim for their teaching in this module, as well as our project mentor Tian Xiao for the advice given to us when working through this project.","title":"Special mentions"},{"location":"dataset/","text":"The dataset we chose was the resale flat prices dataset . This dataset contains the historical transactions of resale Housing Development Board (HDB) flats from 1990 to present, detailing these columns: Column name Description month Sale month of the resale flat town HDB towns; list available here flat_type HDB flat types; list available here block Block number of the resale flat street_name Street name of HDB flat storey_range Range of storeys for HDB flat floor_area_sqm Area of the flat (in square metres) flat_model HDB flat model lease_commencement_date Lease commencement date (YYYY format) remaining_lease Remaining lease length (in years) resale_price Transacted price (in S$) That being said, some of the files do not have the \"remaining lease period\" column, for which we can impute using the assumption that HDB flats have a 99-year lease starting from the lease commencement date, which is provided in all the data files. We believe that the dataset is reliable given that it is provided by the Housing and Development Board and available for public download via the first link above. Furthermore, we can be confident that the data is updated, as is indicated from its metadata given in the same link.","title":"Dataset"},{"location":"eda/","text":"Exploratory data analysis (EDA) is a crucial part in any machine learning project, as it allows one to discover more insights about the data before fully committing to a certain approach in going through the project. Plan # In achieving this goal, we: Collated all our data into a single, unified dataset where all our analysis will be conducted in. Plotted bar charts of the various categories for flat_model , town , and flat_type to detect categories that have very small counts. This is to group these into an other group that represents the minority categories for each feature. Used pandas_profiling to allow for automated EDA report generation. The report is available here . Plots # In grouping the three features into an other category as above, we decided to put the boundaries as (in terms of counts): 9,000 for flat_model 50,000 for flat_type 12,000 for town (note that Sembawang is still included as a separate category) Preliminary analysis # We discovered several features of the dataset, including: High correlation between the floor area, resale price, and lease commencement date. High correlation between flat model, flat type, floor area, and lease commencement date. High correlation between town, flat model, and lease commencement date. Missing remaining lease data for most of the columns, which is imputable using a 99-year HDB lease assumption. These are given by the \\(\\phi_k\\) correlation coefficient which is able to work well with both categorical and numerical columns in the dataset.","title":"Exploratory data analysis"},{"location":"eda/#plan","text":"In achieving this goal, we: Collated all our data into a single, unified dataset where all our analysis will be conducted in. Plotted bar charts of the various categories for flat_model , town , and flat_type to detect categories that have very small counts. This is to group these into an other group that represents the minority categories for each feature. Used pandas_profiling to allow for automated EDA report generation. The report is available here .","title":"Plan"},{"location":"eda/#plots","text":"In grouping the three features into an other category as above, we decided to put the boundaries as (in terms of counts): 9,000 for flat_model 50,000 for flat_type 12,000 for town (note that Sembawang is still included as a separate category)","title":"Plots"},{"location":"eda/#preliminary-analysis","text":"We discovered several features of the dataset, including: High correlation between the floor area, resale price, and lease commencement date. High correlation between flat model, flat type, floor area, and lease commencement date. High correlation between town, flat model, and lease commencement date. Missing remaining lease data for most of the columns, which is imputable using a 99-year HDB lease assumption. These are given by the \\(\\phi_k\\) correlation coefficient which is able to work well with both categorical and numerical columns in the dataset.","title":"Preliminary analysis"},{"location":"evaluations/","text":"This section contains all the data visualisations and/or analyses that we have done for the three best models we have created and selected based on preliminary performance metrics. These performance metrics include the mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), \\(R^2\\) (the coefficient of determination), the mean absolute percentage error (MAPE), and the median absolute percentage error (MdAPE). Rolling XGBoost # xgboost-lat-lon-fig.html","title":"Model evaluation"},{"location":"evaluations/#rolling-xgboost","text":"xgboost-lat-lon-fig.html","title":"Rolling XGBoost"},{"location":"feature-engineering/","text":"Basic feature engineering # We conducted our feature engineering with the aim of producing as many useful features as possible to complement the raw data that has been provided. In order to do this, several things that we have done include: Dropping duplicate transactions Coercing low count categories to OTHER (as described in the exploratory data analysis portion) Converting datetime columns to simpler, numerical columns Extracting the middle floor number from storey_range as this indicates the 'average' value between the range of floors indicated. Remaining lease was imputed using a 99-year lease assumption for HDB flats, which was then transformed into relative_tenure . Using a per-square metre price instead of the resale price itself, due to a high correlation between them. Moreover, we have included a basic Single Ordinary Least Squares (OLS) model to ensure that our data preprocessing is working as intended. This is different from our baseline model, which is a normal linear regression model implemented with Scikit-learn that we will detail more in the Models section. Adding location data # In order to inject more features into the dataset, we decided to use the OneMap API to attain the various ATM, bus stop, hawker centre, library, parks, pharmacies, post office, primary school, store, and train station locations in terms of their latitudes and longitudes. Furthermore, we used the Google Maps API to also geocode the addresses of the HDB resale flats in our dataset. There were, however, some addresses (n = 149) that have failed to be retrieved via geocoding, and some of them (n = 46) also had issues with respect the geocoding that was retrieved, out of a total of 9,307 addresses that were included in the dataset. Although the former was dropped, the latter was included as we decided that the detrimental effect of a 'wrong' geocode was insignificant, and since there might still be useful information to be extracted from other features.","title":"Data preprocessing"},{"location":"feature-engineering/#basic-feature-engineering","text":"We conducted our feature engineering with the aim of producing as many useful features as possible to complement the raw data that has been provided. In order to do this, several things that we have done include: Dropping duplicate transactions Coercing low count categories to OTHER (as described in the exploratory data analysis portion) Converting datetime columns to simpler, numerical columns Extracting the middle floor number from storey_range as this indicates the 'average' value between the range of floors indicated. Remaining lease was imputed using a 99-year lease assumption for HDB flats, which was then transformed into relative_tenure . Using a per-square metre price instead of the resale price itself, due to a high correlation between them. Moreover, we have included a basic Single Ordinary Least Squares (OLS) model to ensure that our data preprocessing is working as intended. This is different from our baseline model, which is a normal linear regression model implemented with Scikit-learn that we will detail more in the Models section.","title":"Basic feature engineering"},{"location":"feature-engineering/#adding-location-data","text":"In order to inject more features into the dataset, we decided to use the OneMap API to attain the various ATM, bus stop, hawker centre, library, parks, pharmacies, post office, primary school, store, and train station locations in terms of their latitudes and longitudes. Furthermore, we used the Google Maps API to also geocode the addresses of the HDB resale flats in our dataset. There were, however, some addresses (n = 149) that have failed to be retrieved via geocoding, and some of them (n = 46) also had issues with respect the geocoding that was retrieved, out of a total of 9,307 addresses that were included in the dataset. Although the former was dropped, the latter was included as we decided that the detrimental effect of a 'wrong' geocode was insignificant, and since there might still be useful information to be extracted from other features.","title":"Adding location data"},{"location":"models/","text":"","title":"Model design and creation"},{"location":"motivation/","text":"Traditional property valuation methods are costly and manual in nature, requiring an appraiser to assess the property and make calculations based on comparable properties. With an automated model that makes price predictions given a set of property characteristics, we have a systematic and unbiased way of getting valuation estimates which are faster and cheaper. Such a model can also reduce fraud cases in valuations, given that the property features that serve as the model's input are not tampered with. Appraisers can use this model to get a first-cut estimate on valuation of properties and supplement it with additional human knowledge of the uniqueness of each property. Furthermore, sellers can use the model to get an estimation of their existing property\u2019s value to arrive at a competitive ask price, while buyers can be more informed to fairly assess their options. In essence, all parties are able to come towards a conclusive price point for a transaction to happen without under- or over-valuing the property.","title":"Motivation"},{"location":"navigate-repo/","text":"data_raw : All the raw data taken from our dataset sources data_processed : Data that has undergone some form of processing docs : Files for documentation website; necessary for MkDocs to function well evaluations : In-depth model evaluations we completed for three selected models, namely a rolling bagging regressor, rolling kNN regressor, and rolling XGBoost regressor. Preliminary performance metrics are computed in each model's Jupyter Notebook instead of here (these are under models ). site : Compiled documentation website after running mkdocs build . Important note: The data files (includes files with extensions .csv , .pkl , and .sav ) that we have used and/or created are not stored in this repo due to storage constraints. This repository is exclusively meant for displaying the work we have completed, and hence we deem the Jupyter Notebooks and figures we have made to be enough proof of such.","title":"Navigating the repository"}]}