{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About # This mini-website is meant as both a short documentation regarding the things our team has done throughout our CS3244 machine learning project, as well as a guide to our GitHub repository that will serve as a copy of its README. Furthermore, this documentation contains the plots accompanying our model evaluations as well as short write-ups to explain said plots. Using the website # Navigate the website by searching (on the top right searchbar) or by clicking any text inside the left navbar. Each page's table of contents will be displayed on the right hand side of the screen. Toggle between light and dark mode by pressing the \"sun\" icon on the top right. CS3244 Team PG26 members # Lim Koon Kiat Ng Wen Jun Ryan Tan Wei Jie S. K. Haridharan Wilson Widyadhana Veronica Angelin Setiyo","title":"Home"},{"location":"#about","text":"This mini-website is meant as both a short documentation regarding the things our team has done throughout our CS3244 machine learning project, as well as a guide to our GitHub repository that will serve as a copy of its README. Furthermore, this documentation contains the plots accompanying our model evaluations as well as short write-ups to explain said plots.","title":"About"},{"location":"#using-the-website","text":"Navigate the website by searching (on the top right searchbar) or by clicking any text inside the left navbar. Each page's table of contents will be displayed on the right hand side of the screen. Toggle between light and dark mode by pressing the \"sun\" icon on the top right.","title":"Using the website"},{"location":"#cs3244-team-pg26-members","text":"Lim Koon Kiat Ng Wen Jun Ryan Tan Wei Jie S. K. Haridharan Wilson Widyadhana Veronica Angelin Setiyo","title":"CS3244 Team PG26 members"},{"location":"credits/","text":"Website # This website was built with Material for MkDocs , a theme for MkDocs . API references # In this project, we have used various libraries and their API documentations. These include, but are not limited to: Scikit-learn XGBoost TensorFlow and its high-level API Keras NumPy Scipy Plotly Express Seaborn Matplotlib Pandas Pandas Profiling Python Documentation Statsmodels OneMap API Google Maps API GitHub Docs We have also used Google Colaboratory to quicken model training and/or evaluation. Resources # Academic research # Predicting the Resale Price of HDB Flats in Singapore (final project for CS5228 20/21 Sem 2): https://www.kaggle.com/misterkix/prediction-of-singapore-hdb-price-machine-learning https://xiongkexin.github.io/assets/report3.pdf Accuracy of Automated Valuation Model for HDB Resale Flats - https://scholarbank.nus.edu.sg/handle/10635/191601?mode=full Housing Price Prediction via Improved Machine Learning Techniques - https://www.sciencedirect.com/science/article/pii/S1877050920316318 Online resources # Towards Data Science (TDS) Medium articles: Predict the Selling Price of HDB Resale Flats (TDS) - https://towardsdatascience.com/predict-the-selling-price-of-hdb-resale-flats-50530391a845 Predicting HDB Housing Prices using Neural Networks (TDS) - https://towardsdatascience.com/predicting-hdb-housing-prices-using-neural-networks-94ab708cccf8 Using Scikit-learn\u2019s Binary Trees to Efficiently Find Latitude and Longitude Neighbors (TDS) - https://towardsdatascience.com/using-scikit-learns-binary-trees-to-efficiently-find-latitude-and-longitude-neighbors-909979bd929b Choosing a scikit-learn estimator (TDS) - https://towardsdatascience.com/choosing-a-scikit-learn-linear-regression-algorithm-dd96b48105f5 StackOverflow (SO) Q&As: How to plot a map with time-slider and zoom on a city with plotly in python (SO) - https://stackoverflow.com/questions/70979065/how-to-plot-a-map-with-time-slider-and-zoom-on-a-city-with-plotly-in-python How to change the colorbar range in plotly express graph (SO) - https://stackoverflow.com/questions/57383651/how-to-change-the-colorbar-range-in-plotly-express-graph Pandas - Number of Months Between Two Dates (SO) - https://stackoverflow.com/questions/42822768/pandas-number-of-months-between-two-dates Annotate data points while plotting from Pandas DataFrame (SO) - https://stackoverflow.com/questions/15910019/annotate-data-points-while-plotting-from-pandas-dataframe Bala\u2019s Curve - https://stackedhomes.com/editorial/freehold-vs-leasehold-part-1/#gs.hbakzn London Relativity - https://www.graphsofrelativity.co.uk/ How to calculate relativity - https://www.savills.com/research_articles/255800/203902-0 Why price per sqm? - https://codyshirk.com/price-per-square-meter-m2-how-we-find-deep-value/ Geocoding addresses - https://www.andrewbreeson.com/projects/geocoding-every-hdb How to Predict HDB Resale Prices using 3 different ML Models (Linear Regression, Neural Networks, k-NN) - https://royleekiat.com/2020/10/22/how-to-predict-hdb-resale-prices-using-3-different-machine-learning-models-linear-regression-neural-networks-k-nearest-neighbours/ Datasets # The datasets that we have used are listed below: HDB resale flat prices dataset MAS core inflation measure (CPI) dataset Singapore overnight rate average (SORA) dataset Special mentions # We would like to thank Prof. Kan Min-Yen and Prof. Brian Lim for their teaching in this module, as well as our project mentor Tian Xiao for the advice given to us when working through this project.","title":"Credits"},{"location":"credits/#website","text":"This website was built with Material for MkDocs , a theme for MkDocs .","title":"Website"},{"location":"credits/#api-references","text":"In this project, we have used various libraries and their API documentations. These include, but are not limited to: Scikit-learn XGBoost TensorFlow and its high-level API Keras NumPy Scipy Plotly Express Seaborn Matplotlib Pandas Pandas Profiling Python Documentation Statsmodels OneMap API Google Maps API GitHub Docs We have also used Google Colaboratory to quicken model training and/or evaluation.","title":"API references"},{"location":"credits/#resources","text":"","title":"Resources"},{"location":"credits/#academic-research","text":"Predicting the Resale Price of HDB Flats in Singapore (final project for CS5228 20/21 Sem 2): https://www.kaggle.com/misterkix/prediction-of-singapore-hdb-price-machine-learning https://xiongkexin.github.io/assets/report3.pdf Accuracy of Automated Valuation Model for HDB Resale Flats - https://scholarbank.nus.edu.sg/handle/10635/191601?mode=full Housing Price Prediction via Improved Machine Learning Techniques - https://www.sciencedirect.com/science/article/pii/S1877050920316318","title":"Academic research"},{"location":"credits/#online-resources","text":"Towards Data Science (TDS) Medium articles: Predict the Selling Price of HDB Resale Flats (TDS) - https://towardsdatascience.com/predict-the-selling-price-of-hdb-resale-flats-50530391a845 Predicting HDB Housing Prices using Neural Networks (TDS) - https://towardsdatascience.com/predicting-hdb-housing-prices-using-neural-networks-94ab708cccf8 Using Scikit-learn\u2019s Binary Trees to Efficiently Find Latitude and Longitude Neighbors (TDS) - https://towardsdatascience.com/using-scikit-learns-binary-trees-to-efficiently-find-latitude-and-longitude-neighbors-909979bd929b Choosing a scikit-learn estimator (TDS) - https://towardsdatascience.com/choosing-a-scikit-learn-linear-regression-algorithm-dd96b48105f5 StackOverflow (SO) Q&As: How to plot a map with time-slider and zoom on a city with plotly in python (SO) - https://stackoverflow.com/questions/70979065/how-to-plot-a-map-with-time-slider-and-zoom-on-a-city-with-plotly-in-python How to change the colorbar range in plotly express graph (SO) - https://stackoverflow.com/questions/57383651/how-to-change-the-colorbar-range-in-plotly-express-graph Pandas - Number of Months Between Two Dates (SO) - https://stackoverflow.com/questions/42822768/pandas-number-of-months-between-two-dates Annotate data points while plotting from Pandas DataFrame (SO) - https://stackoverflow.com/questions/15910019/annotate-data-points-while-plotting-from-pandas-dataframe Bala\u2019s Curve - https://stackedhomes.com/editorial/freehold-vs-leasehold-part-1/#gs.hbakzn London Relativity - https://www.graphsofrelativity.co.uk/ How to calculate relativity - https://www.savills.com/research_articles/255800/203902-0 Why price per sqm? - https://codyshirk.com/price-per-square-meter-m2-how-we-find-deep-value/ Geocoding addresses - https://www.andrewbreeson.com/projects/geocoding-every-hdb How to Predict HDB Resale Prices using 3 different ML Models (Linear Regression, Neural Networks, k-NN) - https://royleekiat.com/2020/10/22/how-to-predict-hdb-resale-prices-using-3-different-machine-learning-models-linear-regression-neural-networks-k-nearest-neighbours/","title":"Online resources"},{"location":"credits/#datasets","text":"The datasets that we have used are listed below: HDB resale flat prices dataset MAS core inflation measure (CPI) dataset Singapore overnight rate average (SORA) dataset","title":"Datasets"},{"location":"credits/#special-mentions","text":"We would like to thank Prof. Kan Min-Yen and Prof. Brian Lim for their teaching in this module, as well as our project mentor Tian Xiao for the advice given to us when working through this project.","title":"Special mentions"},{"location":"dataset/","text":"The dataset we chose was the resale flat prices dataset . This dataset contains the historical transactions of resale Housing Development Board (HDB) flats from 1990 to present, detailing these columns: Column name Description month Sale month of the resale flat town HDB towns; list available here flat_type HDB flat types; list available here block Block number of the resale flat street_name Street name of HDB flat storey_range Range of storeys for HDB flat floor_area_sqm Area of the flat (in square metres) flat_model HDB flat model lease_commencement_date Lease commencement date (YYYY format) remaining_lease Remaining lease length (in years) resale_price Transacted price (in S$) That being said, some of the files do not have the \"remaining lease period\" column, for which we can impute using the assumption that HDB flats have a 99-year lease starting from the lease commencement date, which is provided in all the data files. We believe that the dataset is reliable given that it is provided by the Housing and Development Board and available for public download via the first link above. Furthermore, we can be confident that the data is updated, as is indicated from its metadata given in the same link.","title":"Dataset"},{"location":"eda/","text":"Exploratory data analysis (EDA) is a crucial part in any machine learning project, as it allows one to discover more insights about the data before fully committing to a certain approach in going through the project. Plan # In achieving this goal, we: Collated all our data into a single, unified dataset where all our analysis will be conducted in. Plotted bar charts of the various categories for flat_model , town , and flat_type to detect categories that have very small counts. This is to group these into an other group that represents the minority categories for each feature. Used pandas_profiling to allow for automated EDA report generation. The report is available here . Plots # In grouping the three features into an other category as above, we decided to put the boundaries as (in terms of counts): 9,000 for flat_model 50,000 for flat_type 12,000 for town (note that Sembawang is still included as a separate category) Preliminary analysis # We discovered several features of the dataset, including: High correlation between the floor area, resale price, and lease commencement date. High correlation between flat model, flat type, floor area, and lease commencement date. High correlation between town, flat model, and lease commencement date. Missing remaining lease data for most of the columns, which is imputable using a 99-year HDB lease assumption. These are given by the \\(\\phi_k\\) correlation coefficient which is able to work well with both categorical and numerical columns in the dataset.","title":"Exploratory data analysis"},{"location":"eda/#plan","text":"In achieving this goal, we: Collated all our data into a single, unified dataset where all our analysis will be conducted in. Plotted bar charts of the various categories for flat_model , town , and flat_type to detect categories that have very small counts. This is to group these into an other group that represents the minority categories for each feature. Used pandas_profiling to allow for automated EDA report generation. The report is available here .","title":"Plan"},{"location":"eda/#plots","text":"In grouping the three features into an other category as above, we decided to put the boundaries as (in terms of counts): 9,000 for flat_model 50,000 for flat_type 12,000 for town (note that Sembawang is still included as a separate category)","title":"Plots"},{"location":"eda/#preliminary-analysis","text":"We discovered several features of the dataset, including: High correlation between the floor area, resale price, and lease commencement date. High correlation between flat model, flat type, floor area, and lease commencement date. High correlation between town, flat model, and lease commencement date. Missing remaining lease data for most of the columns, which is imputable using a 99-year HDB lease assumption. These are given by the \\(\\phi_k\\) correlation coefficient which is able to work well with both categorical and numerical columns in the dataset.","title":"Preliminary analysis"},{"location":"evaluations/","text":"General description # The following evaluation metrics are options that we have considered. However, to judge the efficacy of our models, we have decided on primarily using the mean absolute percentage error (MAPE) and median absolute percentage error (MdAPE). These evaluation metrics are best suited for our particular dataset due to property prices having a wider range of values, i.e. a percentage error indicates a similar proportion of impact be it for a $100,000 or a $10,000,000 property. This is crucial as the resale price has a heavy right-tailed distribution (see the figure below; also available in the EDA report) due to some properties having very high transaction prices. Furthermore, both the MAPE and the MdAPE are more easily understandable metrics as they are described in terms of percentages. Price histogram # Below is a histogram of resale property prices, which indicates a heavy right-tailed distribution. Primary model evaluations # In this section, we shall detail the performance metrics of the models that our team has created. Note that since this is only a primary stage of evaluation, we only compare the different models based on the five metrics that we have chosen, without any accompanying data visualisations. Biased model results # Please visit this page to view the performance metrics of our models that have been trained using an arbitrary train-test split, i.e., the results of models tainted with lookahead bias. For more details this issue and our solution, please visit this page . A notable result is that of kNN regressor, where the MAPE was close to 0%, indicating that there must be some serious issue either with overfitting or with the training data. Unbiased model results # Please visit this page to view the performance metrics of our models trained after fixing the lookahead bias. As we can see, the best models are (in order of increasing MdAPE) rolling bagging regressor, rolling kNN regressor, and rolling XGBoost regressor. Hence we shall choose these three models for further analysis below. Description of selected models # Rolling bagging regressor # The first of our models is the rolling bagging regressor. This model is an ensemble which fits a base regressor on random subsets of the original dataset before aggregating their individual predictions (either by voting or by averaging) to form a final prediction. The base regressor used was the default decision tree regressor. However, when used alone, the decision tree regressor results in high variance in the data. Hence, we chose to use an ensemble of decision trees to reduce the variance associated through randomisation and aggregation of results. Overall, the rolling bagging regressor performed the best out of the three models we chose, with an MAPE of 6.30% and a MdAPE of 4.71%. Rolling kNN regressor # The second selected model is the rolling kNN Regressor. This model averages the prices of the k nearest neighbours of every target to provide a prediction for the target\u2019s price. The rolling kNN regressor model was chosen because of its simplicity as well as its stellar results. The rolling kNN regressor performed the best out of the three models chosen, with an MAPE of 6.41% and MdAPE of 4.73%. Rolling XGBoost regressor # Our final model is the XGBoost regressor. This model is also an ensemble which uses decision trees as the base regressor on random subsets before aggregating individual predictions to form a final prediction. However, the key differentiator of the XGBoost regressor is that the XGBoost algorithm utilises an optimised version of gradient boosting. Boosting, in this sense, involves building base estimators sequentially with the aim of reducing the bias of the combined estimator. Gradient boosting expands on this by minimising the loss function of particular learners within the model. XGBoost then further optimises on gradient boosting by using L1 and L2 regularisation, which prevents overfitting and improves generalisation capabilities. As expected, the rolling XGBoost regressor also performed superbly, with an MAPE of 6.50% and a MdAPE of 4.83%, which is slightly worse than the other two models we chose. Extended model evaluations # In this section, we shall detail the three models that we have selected, namely rolling bagging regressor, rolling kNN regressor, and rolling XGBoost regressor in more depth through the use of data visualisations on the models' predictions. All models perform poorly at the start of the period (which is in 1991) according to the MAPE and MdAPE metrics due to a lack of data from previous time intervals. In several towns, most notably Sembawang, the models perform exceptionally poorly across certain time periods due to sparsity of data in those time periods. For example, the number of data points each year for Sembawang is less than 10 from 1990 to 2001, with many years having only 2 data points. This lack of transaction data can be seen with this visualisation . It can also be noted that across all chosen models, the mean absolute error is the highest for towns near the southern region of central Singapore, such as Bukit Merah (7.19/7.55/7.51), Queenstown (7.49/7.90/7.70) and Kallang (7.38/7.56/7.68) (from left to right: rolling bagging regressor, rolling XGBoost regressor, rolling kNN regressor). This is most likely due to the smaller number of datapoints for towns in said region across the entire timeframe, as indicated by the size of the bubbles in this figure . We list out the visualisations that support the above points below: Per-HDB yearly scatter mapbox Per-town yearly scatter mapbox Rolling bagging regressor Rolling kNN regressor Rolling XGBoost regressor Per-HDB scatter mapbox of MAPEs baggingregressor-lat-lon-fig knnregressor-lat-lon-fig xgboost-lat-lon-fig Per-town scatter mapbox of MAPEs baggingregressor-town-fig knnregressor-town-fig xgboost-town-fig Per-town scatter mapbox of MdAPEs baggingregressor-town-fig-med knnregressor-town-fig-med xgboost-town-fig-med Per-town yearly scatter mapbox of MAPEs baggingregressor-time-town-fig knnregressor-time-town-fig xgboost-time-town-fig Per-town yearly line plot of MAPEs baggingregressor-mape-time-town knnregressor-mape-time-town xgboost-mape-time-town Per-town yearly line plot of MdAPEs baggingregressor-mdape-time-town knnregressor-mdape-time-town xgboost-mdape-time-town Per-town yearly boxplot of MAPEs baggingregressor-mape-boxplot-town knnregressor-mape-boxplot-town xgboost-mape-boxplot-town Heatmap of features ( actual = target variable, pred = prediction) baggingregressor-heatmap knnregressor-heatmap xgboost-heatmap Per-town scatterplot of MAPEs (red line is overall model MAPE) baggingregressor-town-mape knnregressor-town-mape xgboost-town-mape","title":"Model evaluation"},{"location":"evaluations/#general-description","text":"The following evaluation metrics are options that we have considered. However, to judge the efficacy of our models, we have decided on primarily using the mean absolute percentage error (MAPE) and median absolute percentage error (MdAPE). These evaluation metrics are best suited for our particular dataset due to property prices having a wider range of values, i.e. a percentage error indicates a similar proportion of impact be it for a $100,000 or a $10,000,000 property. This is crucial as the resale price has a heavy right-tailed distribution (see the figure below; also available in the EDA report) due to some properties having very high transaction prices. Furthermore, both the MAPE and the MdAPE are more easily understandable metrics as they are described in terms of percentages.","title":"General description"},{"location":"evaluations/#price-histogram","text":"Below is a histogram of resale property prices, which indicates a heavy right-tailed distribution.","title":"Price histogram"},{"location":"evaluations/#primary-model-evaluations","text":"In this section, we shall detail the performance metrics of the models that our team has created. Note that since this is only a primary stage of evaluation, we only compare the different models based on the five metrics that we have chosen, without any accompanying data visualisations.","title":"Primary model evaluations"},{"location":"evaluations/#biased-model-results","text":"Please visit this page to view the performance metrics of our models that have been trained using an arbitrary train-test split, i.e., the results of models tainted with lookahead bias. For more details this issue and our solution, please visit this page . A notable result is that of kNN regressor, where the MAPE was close to 0%, indicating that there must be some serious issue either with overfitting or with the training data.","title":"Biased model results"},{"location":"evaluations/#unbiased-model-results","text":"Please visit this page to view the performance metrics of our models trained after fixing the lookahead bias. As we can see, the best models are (in order of increasing MdAPE) rolling bagging regressor, rolling kNN regressor, and rolling XGBoost regressor. Hence we shall choose these three models for further analysis below.","title":"Unbiased model results"},{"location":"evaluations/#description-of-selected-models","text":"","title":"Description of selected models"},{"location":"evaluations/#rolling-bagging-regressor","text":"The first of our models is the rolling bagging regressor. This model is an ensemble which fits a base regressor on random subsets of the original dataset before aggregating their individual predictions (either by voting or by averaging) to form a final prediction. The base regressor used was the default decision tree regressor. However, when used alone, the decision tree regressor results in high variance in the data. Hence, we chose to use an ensemble of decision trees to reduce the variance associated through randomisation and aggregation of results. Overall, the rolling bagging regressor performed the best out of the three models we chose, with an MAPE of 6.30% and a MdAPE of 4.71%.","title":"Rolling bagging regressor"},{"location":"evaluations/#rolling-knn-regressor","text":"The second selected model is the rolling kNN Regressor. This model averages the prices of the k nearest neighbours of every target to provide a prediction for the target\u2019s price. The rolling kNN regressor model was chosen because of its simplicity as well as its stellar results. The rolling kNN regressor performed the best out of the three models chosen, with an MAPE of 6.41% and MdAPE of 4.73%.","title":"Rolling kNN regressor"},{"location":"evaluations/#rolling-xgboost-regressor","text":"Our final model is the XGBoost regressor. This model is also an ensemble which uses decision trees as the base regressor on random subsets before aggregating individual predictions to form a final prediction. However, the key differentiator of the XGBoost regressor is that the XGBoost algorithm utilises an optimised version of gradient boosting. Boosting, in this sense, involves building base estimators sequentially with the aim of reducing the bias of the combined estimator. Gradient boosting expands on this by minimising the loss function of particular learners within the model. XGBoost then further optimises on gradient boosting by using L1 and L2 regularisation, which prevents overfitting and improves generalisation capabilities. As expected, the rolling XGBoost regressor also performed superbly, with an MAPE of 6.50% and a MdAPE of 4.83%, which is slightly worse than the other two models we chose.","title":"Rolling XGBoost regressor"},{"location":"evaluations/#extended-model-evaluations","text":"In this section, we shall detail the three models that we have selected, namely rolling bagging regressor, rolling kNN regressor, and rolling XGBoost regressor in more depth through the use of data visualisations on the models' predictions. All models perform poorly at the start of the period (which is in 1991) according to the MAPE and MdAPE metrics due to a lack of data from previous time intervals. In several towns, most notably Sembawang, the models perform exceptionally poorly across certain time periods due to sparsity of data in those time periods. For example, the number of data points each year for Sembawang is less than 10 from 1990 to 2001, with many years having only 2 data points. This lack of transaction data can be seen with this visualisation . It can also be noted that across all chosen models, the mean absolute error is the highest for towns near the southern region of central Singapore, such as Bukit Merah (7.19/7.55/7.51), Queenstown (7.49/7.90/7.70) and Kallang (7.38/7.56/7.68) (from left to right: rolling bagging regressor, rolling XGBoost regressor, rolling kNN regressor). This is most likely due to the smaller number of datapoints for towns in said region across the entire timeframe, as indicated by the size of the bubbles in this figure . We list out the visualisations that support the above points below: Per-HDB yearly scatter mapbox Per-town yearly scatter mapbox Rolling bagging regressor Rolling kNN regressor Rolling XGBoost regressor Per-HDB scatter mapbox of MAPEs baggingregressor-lat-lon-fig knnregressor-lat-lon-fig xgboost-lat-lon-fig Per-town scatter mapbox of MAPEs baggingregressor-town-fig knnregressor-town-fig xgboost-town-fig Per-town scatter mapbox of MdAPEs baggingregressor-town-fig-med knnregressor-town-fig-med xgboost-town-fig-med Per-town yearly scatter mapbox of MAPEs baggingregressor-time-town-fig knnregressor-time-town-fig xgboost-time-town-fig Per-town yearly line plot of MAPEs baggingregressor-mape-time-town knnregressor-mape-time-town xgboost-mape-time-town Per-town yearly line plot of MdAPEs baggingregressor-mdape-time-town knnregressor-mdape-time-town xgboost-mdape-time-town Per-town yearly boxplot of MAPEs baggingregressor-mape-boxplot-town knnregressor-mape-boxplot-town xgboost-mape-boxplot-town Heatmap of features ( actual = target variable, pred = prediction) baggingregressor-heatmap knnregressor-heatmap xgboost-heatmap Per-town scatterplot of MAPEs (red line is overall model MAPE) baggingregressor-town-mape knnregressor-town-mape xgboost-town-mape","title":"Extended model evaluations"},{"location":"feature-engineering/","text":"General description # We conducted our feature engineering with the aim of producing as many useful features as possible to complement the raw data that has been provided. Data processing # Dropping duplicate transactions Coercing low count categories to OTHER (as described in the exploratory data analysis portion) Converting datetime columns to simpler, numerical columns Extracting the middle floor number from storey_range as this indicates the 'average' value between the range of floors indicated. Remaining lease was imputed using a 99-year lease assumption for HDB flats, which was then transformed into relative_tenure . Using a per-square metre price instead of the resale price itself, due to a high correlation between them. Feature engineering # Target variable # We used resale_price and floor_area to compute the CPI-adjusted price-per square metre ( cpi_psm ) which should be used as an indicator of the underlying value of a property. It is the easiest way to ensure that we have an apple to apples comparison when comparing properties\u2019 values within a market. Predictors # Feature Type Engineering Reason floor NC Since the data does not provide the exact floor, we take the middle value of the storey_range column. I age NC We computed the building\u2019s age using sale_date - lease_commence_date I relative_tenure NC remaining_lease was imputed using the fact that all HDB properties have a 99-year lease. This was then transformed into relative_tenure. This measure of relativity relates the term left for a property to the percentage of freehold value by curve fitting. The equation is set up as a discount formula and minimises the sum of squared errors. See curve and formula below. I floor_area NC Floor area of property in square metres. I nearest_<point of interest> NC nearest_<point of interest> features using a property\u2019s coordinates and point of interest coordinates (obtained using OneMapAPI and Google Maps API - see below table). The distances are in kilometres. Our hypothesis is that the nearby amenities and points of interest will be significant in a property\u2019s value. sale_month ND Converting datetime columns to an encoded numerical feature sale_month for our single models. sale_month assigns each month with a number and it\u2019s ordered from 1 for the earliest month and increments up to the latest month. Valuations are time varying dependent on the economy, the housing market and the government\u2019s policies. cpi NC Data provided by the Monetary Authority of Singapore (MAS) Since our data spans a long time period, correcting resale prices to the present requires the use of CPI data. avg_sora NC Data provided by MAS We hypothesise that interest rates have an effect on the prices of homes. is_imputed_sora CB An indicator on whether the avg_sora for that observation was imputed with the mean. Additional information for the model to infer whether it is an observed sora or imputed. flat_type CN Type of HDB flat. Low counts were coerced to OTHER . Cutoff ( \\(>= 50000\\) ) chosen with elbow plot. EDA flat_model CN Flat model. Low counts were coerced to OTHER . Cutoff ( \\(>= 9000\\) ) chosen with elbow plot. EDA town CN Town where the property is located in. Low counts were coerced to OTHER. Cutoff ( \\(>= 12000\\) ) chosen with elbow plot. EDA period CO period for our rolling models. period is a quarterly label e.g. 1991Q1 , 1991Q2 , 1991Q3 , 1991Q4 , 1992Q1 , etc. Same as sale_month but different representation for rolling models. Abbreviations used: # NC = Numerical (continuous) I = Well-known and undamentally important features that determines a property's value. ND = Numerical (discrete) CB = Categorical (boolean) CN = Categorical (nominal) EDA = Based on EDA done (see section on EDA for more complete explanations) CO = Categorical (ordinal) Relativity measure for tenure # Adding location data # In order to inject more features into the dataset, we decided to use the OneMap API to attain the various ATM, bus stop, hawker centre, library, parks, pharmacies, post office, primary school, store, and train station locations in terms of their latitudes and longitudes. Furthermore, we used the Google Maps API to also geocode the addresses of the HDB resale flats in our dataset. There were, however, some addresses (n = 149) that have failed to be retrieved via geocoding, and some of them (n = 46) also had issues with respect the geocoding that was retrieved, out of a total of 9,307 unique HDB flat addresses that were included in the dataset. Although the former was dropped, the latter was included as we decided that the detrimental effect of a 'wrong' geocode was insignificant, and since there might still be useful information to be extracted from other features.","title":"Data preprocessing"},{"location":"feature-engineering/#general-description","text":"We conducted our feature engineering with the aim of producing as many useful features as possible to complement the raw data that has been provided.","title":"General description"},{"location":"feature-engineering/#data-processing","text":"Dropping duplicate transactions Coercing low count categories to OTHER (as described in the exploratory data analysis portion) Converting datetime columns to simpler, numerical columns Extracting the middle floor number from storey_range as this indicates the 'average' value between the range of floors indicated. Remaining lease was imputed using a 99-year lease assumption for HDB flats, which was then transformed into relative_tenure . Using a per-square metre price instead of the resale price itself, due to a high correlation between them.","title":"Data processing"},{"location":"feature-engineering/#feature-engineering","text":"","title":"Feature engineering"},{"location":"feature-engineering/#target-variable","text":"We used resale_price and floor_area to compute the CPI-adjusted price-per square metre ( cpi_psm ) which should be used as an indicator of the underlying value of a property. It is the easiest way to ensure that we have an apple to apples comparison when comparing properties\u2019 values within a market.","title":"Target variable"},{"location":"feature-engineering/#predictors","text":"Feature Type Engineering Reason floor NC Since the data does not provide the exact floor, we take the middle value of the storey_range column. I age NC We computed the building\u2019s age using sale_date - lease_commence_date I relative_tenure NC remaining_lease was imputed using the fact that all HDB properties have a 99-year lease. This was then transformed into relative_tenure. This measure of relativity relates the term left for a property to the percentage of freehold value by curve fitting. The equation is set up as a discount formula and minimises the sum of squared errors. See curve and formula below. I floor_area NC Floor area of property in square metres. I nearest_<point of interest> NC nearest_<point of interest> features using a property\u2019s coordinates and point of interest coordinates (obtained using OneMapAPI and Google Maps API - see below table). The distances are in kilometres. Our hypothesis is that the nearby amenities and points of interest will be significant in a property\u2019s value. sale_month ND Converting datetime columns to an encoded numerical feature sale_month for our single models. sale_month assigns each month with a number and it\u2019s ordered from 1 for the earliest month and increments up to the latest month. Valuations are time varying dependent on the economy, the housing market and the government\u2019s policies. cpi NC Data provided by the Monetary Authority of Singapore (MAS) Since our data spans a long time period, correcting resale prices to the present requires the use of CPI data. avg_sora NC Data provided by MAS We hypothesise that interest rates have an effect on the prices of homes. is_imputed_sora CB An indicator on whether the avg_sora for that observation was imputed with the mean. Additional information for the model to infer whether it is an observed sora or imputed. flat_type CN Type of HDB flat. Low counts were coerced to OTHER . Cutoff ( \\(>= 50000\\) ) chosen with elbow plot. EDA flat_model CN Flat model. Low counts were coerced to OTHER . Cutoff ( \\(>= 9000\\) ) chosen with elbow plot. EDA town CN Town where the property is located in. Low counts were coerced to OTHER. Cutoff ( \\(>= 12000\\) ) chosen with elbow plot. EDA period CO period for our rolling models. period is a quarterly label e.g. 1991Q1 , 1991Q2 , 1991Q3 , 1991Q4 , 1992Q1 , etc. Same as sale_month but different representation for rolling models.","title":"Predictors"},{"location":"feature-engineering/#abbreviations-used","text":"NC = Numerical (continuous) I = Well-known and undamentally important features that determines a property's value. ND = Numerical (discrete) CB = Categorical (boolean) CN = Categorical (nominal) EDA = Based on EDA done (see section on EDA for more complete explanations) CO = Categorical (ordinal)","title":"Abbreviations used:"},{"location":"feature-engineering/#relativity-measure-for-tenure","text":"","title":"Relativity measure for tenure"},{"location":"feature-engineering/#adding-location-data","text":"In order to inject more features into the dataset, we decided to use the OneMap API to attain the various ATM, bus stop, hawker centre, library, parks, pharmacies, post office, primary school, store, and train station locations in terms of their latitudes and longitudes. Furthermore, we used the Google Maps API to also geocode the addresses of the HDB resale flats in our dataset. There were, however, some addresses (n = 149) that have failed to be retrieved via geocoding, and some of them (n = 46) also had issues with respect the geocoding that was retrieved, out of a total of 9,307 unique HDB flat addresses that were included in the dataset. Although the former was dropped, the latter was included as we decided that the detrimental effect of a 'wrong' geocode was insignificant, and since there might still be useful information to be extracted from other features.","title":"Adding location data"},{"location":"feature-importance/","text":"We\u2019ll focus our discussion of feature importance using the XGBoost single model (the first figure below) for simplicity. The other feature importance plots are from our decision tree with recursive feature elimination (RFE), decision tree with AdaBoost, and decision tree with bagging regressor, in that order. Observations # (click on the figures to zoom in) The flat attributes features, such as sale_month , floor , age , floor_area , have the highest importance. This is because these are well-known and fundamental features that determine a property\u2019s value. Next on importance are the economic indicator avg_sora , which indicates Singapore\u2019s interest rate for the year, as well as cpi, which is the consumer price index of the current period. Next on importance are the points of interest such as ATM\u2019s, MRT and bus stops, hawker centres and schools. Looking across the feature importance charts for XGBoost, decision tree with RFE, decision tree with AdaBoost, and decision tree with bagging regressor, they generally come to the same conclusion in feature importance, with a few of the more important features switching places with each other.","title":"Feature importance analysis"},{"location":"feature-importance/#observations","text":"(click on the figures to zoom in) The flat attributes features, such as sale_month , floor , age , floor_area , have the highest importance. This is because these are well-known and fundamental features that determine a property\u2019s value. Next on importance are the economic indicator avg_sora , which indicates Singapore\u2019s interest rate for the year, as well as cpi, which is the consumer price index of the current period. Next on importance are the points of interest such as ATM\u2019s, MRT and bus stops, hawker centres and schools. Looking across the feature importance charts for XGBoost, decision tree with RFE, decision tree with AdaBoost, and decision tree with bagging regressor, they generally come to the same conclusion in feature importance, with a few of the more important features switching places with each other.","title":"Observations"},{"location":"future-improvements/","text":"Our project certainly has a few possible future improvements that are worthy of being implemented. These include: Using a voting regressor, which uses several distinct models to 'vote' for property prices hence allowing us to amalgamate several models together. Developing additional methods for model explainability, such as visualisations of the data points for kNN regressor. Ensembling other base regressors besides decision trees. Adding hyperparameter tuning for our rolling models, which may improve their performance metrics.","title":"Future improvements"},{"location":"lookahead-bias/","text":"After looking at how the data was split, we realised that we have induced a lookahead bias in our models, as seen via the results from our biassed kNN regressor model's predictions (see below), where information that would have been unknown in a certain time period was being analysed. This was because our train test split was done by arbitrary sampling, which meant that the model learnt some mathematical relation to predict resale flat prices based on the sale month of the datapoint. To fix this issue, we took the most recent 4 years of data as our test set and our training set to be everything before said period. This way, the model would not have access to any data in the test set, which is in a future time period. After correcting for the lookahead bias and retraining our models, our team managed to produce a result that seems more reasonable. Note that this issue is only present in the non-rolling version of the models, as the rolling version uses a rolling period and instead of a simple train-test split. Biased kNN regressor predictions # We can see that the predictions are almost perfect, as indicated by the predictions following the red line very closely. Furthermore, the MAPE and MdAPE are 0.04% and 0.03% respectively, which confirms the near-perfect predictions and the lookahead bias present in the model.","title":"Lookahead bias"},{"location":"lookahead-bias/#biased-knn-regressor-predictions","text":"We can see that the predictions are almost perfect, as indicated by the predictions following the red line very closely. Furthermore, the MAPE and MdAPE are 0.04% and 0.03% respectively, which confirms the near-perfect predictions and the lookahead bias present in the model.","title":"Biased kNN regressor predictions"},{"location":"models/","text":"Baseline models # We initially created a simple linear regression and polynomial regression model for our baseline as they are versatile and allow us to understand the data better. For these two models, the data was split into an 80% training and 20% test set, and from our results, linear regression doesn\u2019t perform too well, with an MAPE of 27% and 16% respectively. We can see that a polynomial regression model with degree 2 performs slightly better, however the performance increase is only marginal at best. This led us to explore other types of models such as decision trees, neural networks, kNN regressor, and ensemble models, the designs of which we iterated based on their performance. More advanced models # As mentioned above, we decided to implement several types of models, namely decision tree regressors, kNN regressor, neural networks, and ensemble regressors. The first and last fall into the symbolist tribe, the second into the analogizer, and the third into the connectionist. With the exception of neural networks, we used a grid search algorithm on all our model types to automatically find the best hyperparameter(s) out of the possibilities we gave in the hyperparameter search space. Each of the types of models we created also have rolling versions that use a rolling time period for training to act as a time-based cross-validation method. Decision trees (and its ensembles) # We first made a basic decision tree model, after which we improved its features by adding on recursive feature elimination, as well as modified the learning algorithm itself by exploiting ensemble learning. The latter is done with adaptive boosting regressors (in AdaBoost) or by selecting random subsets of the data and fitting base regressors on them (as is the case in a bagging regressor). Both of these, however, are done with decision trees as the base regressor. Neural networks # We first created a network with a 54-1 dense architecture to act as a baseline for this model type. This is then enhanced into a 54-108-108-1 architecture, which immediately led to major overfitting problems. After some tweaking and fixing of the lookahead bias , we eventually settled on a 57-20-1 architecture. This, however, had extremely inaccurate predictions on a level comparable to the simple linear regression we developed earlier. kNN regressor # In creating this model, we decided to grid search on a 10% sample of the original dataset to quicken our search of an optimal hyperparameter. This grid search includes finding the best distance metric, value of \\(k\\) , as well as whether the points are weighted uniformly or by the inverse of their distance. Ensemble regressors # These regressors are built on top of base regressors and extended either via \"boosting\" (as is the case with AdaBoost and XGBoost) or \"bagging\" (as is the case with bagging regressor). AdaBoost does so by fitting more weight-adjusted (based on the current error) copies of the regressor on top of the base, while the bagging regressor samples random subsets of the dataset and trains copies of the base regressor on it, before predicting via averaging each copy's predictions. XGBoost is similar to AdaBoost albeit with an optimised version of gradient boosting instead of normal boosting. Rolling models # We were motivated by 2 main observations for pursuing periodically rolling models: Property values observed in 1991 may not be as informative about the values observed in 2012 compared to the values in 2011 or 2013. The real estate market, much like the stock market, undergoes \u201cregimes\u201d. For instance, in the 2008-2009 financial crisis, the market crashed. We want to localise the time frame, train in a batch of 5 quarters, and use the 6th quarter to validate our mini-model. Then we perform this action to slide through our entire dataset. All these mini-models will then be part of our final model. We observe significant performance improvement after adopting this approach, e.g., the single XGBoost model has MAPE and MdAPE of 9.0% and 7.5% respectively, while the rolling variant has 6.5% and 4.8% respectively, even without hyperparameter tuning. It should also be noted that we did perform some hyperparameter tuning for the single models but none for the rolling models. Hence, we can potentially expect some further improvements in performance if we were to tune the rolling models.","title":"Model design and creation"},{"location":"models/#baseline-models","text":"We initially created a simple linear regression and polynomial regression model for our baseline as they are versatile and allow us to understand the data better. For these two models, the data was split into an 80% training and 20% test set, and from our results, linear regression doesn\u2019t perform too well, with an MAPE of 27% and 16% respectively. We can see that a polynomial regression model with degree 2 performs slightly better, however the performance increase is only marginal at best. This led us to explore other types of models such as decision trees, neural networks, kNN regressor, and ensemble models, the designs of which we iterated based on their performance.","title":"Baseline models"},{"location":"models/#more-advanced-models","text":"As mentioned above, we decided to implement several types of models, namely decision tree regressors, kNN regressor, neural networks, and ensemble regressors. The first and last fall into the symbolist tribe, the second into the analogizer, and the third into the connectionist. With the exception of neural networks, we used a grid search algorithm on all our model types to automatically find the best hyperparameter(s) out of the possibilities we gave in the hyperparameter search space. Each of the types of models we created also have rolling versions that use a rolling time period for training to act as a time-based cross-validation method.","title":"More advanced models"},{"location":"models/#decision-trees-and-its-ensembles","text":"We first made a basic decision tree model, after which we improved its features by adding on recursive feature elimination, as well as modified the learning algorithm itself by exploiting ensemble learning. The latter is done with adaptive boosting regressors (in AdaBoost) or by selecting random subsets of the data and fitting base regressors on them (as is the case in a bagging regressor). Both of these, however, are done with decision trees as the base regressor.","title":"Decision trees (and its ensembles)"},{"location":"models/#neural-networks","text":"We first created a network with a 54-1 dense architecture to act as a baseline for this model type. This is then enhanced into a 54-108-108-1 architecture, which immediately led to major overfitting problems. After some tweaking and fixing of the lookahead bias , we eventually settled on a 57-20-1 architecture. This, however, had extremely inaccurate predictions on a level comparable to the simple linear regression we developed earlier.","title":"Neural networks"},{"location":"models/#knn-regressor","text":"In creating this model, we decided to grid search on a 10% sample of the original dataset to quicken our search of an optimal hyperparameter. This grid search includes finding the best distance metric, value of \\(k\\) , as well as whether the points are weighted uniformly or by the inverse of their distance.","title":"kNN regressor"},{"location":"models/#ensemble-regressors","text":"These regressors are built on top of base regressors and extended either via \"boosting\" (as is the case with AdaBoost and XGBoost) or \"bagging\" (as is the case with bagging regressor). AdaBoost does so by fitting more weight-adjusted (based on the current error) copies of the regressor on top of the base, while the bagging regressor samples random subsets of the dataset and trains copies of the base regressor on it, before predicting via averaging each copy's predictions. XGBoost is similar to AdaBoost albeit with an optimised version of gradient boosting instead of normal boosting.","title":"Ensemble regressors"},{"location":"models/#rolling-models","text":"We were motivated by 2 main observations for pursuing periodically rolling models: Property values observed in 1991 may not be as informative about the values observed in 2012 compared to the values in 2011 or 2013. The real estate market, much like the stock market, undergoes \u201cregimes\u201d. For instance, in the 2008-2009 financial crisis, the market crashed. We want to localise the time frame, train in a batch of 5 quarters, and use the 6th quarter to validate our mini-model. Then we perform this action to slide through our entire dataset. All these mini-models will then be part of our final model. We observe significant performance improvement after adopting this approach, e.g., the single XGBoost model has MAPE and MdAPE of 9.0% and 7.5% respectively, while the rolling variant has 6.5% and 4.8% respectively, even without hyperparameter tuning. It should also be noted that we did perform some hyperparameter tuning for the single models but none for the rolling models. Hence, we can potentially expect some further improvements in performance if we were to tune the rolling models.","title":"Rolling models"},{"location":"motivation/","text":"Background # Traditional property valuation methods are costly and manual in nature, requiring an appraiser to assess the property and make calculations based on comparable properties. With an automated model that makes price predictions given a set of property characteristics, we have a systematic and unbiased way of getting valuation estimates which are faster and cheaper. Such a model can also reduce fraud cases in valuations, given that the property features that serve as the model's inputs are not tampered with. Appraisers can use this model to get a first-cut estimate on valuation of properties and supplement it with additional human knowledge of the uniqueness of each property. Furthermore, sellers can use the model to get an estimation of their existing property\u2019s value to arrive at a competitive ask price, while buyers can be more informed to fairly assess their options. In essence, all parties are able to come towards a conclusive price point for a transaction to happen without under- or over-valuing the property. Main goals # With our motivation in mind, we wanted to do three things in this project: Making an accurate price prediction model # First, we aim to create an ML model that can accurately make price predictions given property characteristics. Our plan was to try out different machine learning models that would be suitable for this task and dataset, and then evaluate their performance to determine the best models. Feature importance analysis # We also aim to identify the features most responsible for a property's resale price, to derive interesting observations from our model's performance on the dataset. This will be informative to home buyers on what are the most influential features that will affect a HDB flat\u2019s value. Identify under- and over-valued properties using model predictions # Lastly, we also wanted to run a simulation to study how well our model would perform if we were to use its predictions to identify undervalued/overvalued properties on a rolling basis. This is to say, if someone were to use our model to purchase an identified undervalued property, would they see a good return on investment after 10 years?","title":"Motivation"},{"location":"motivation/#background","text":"Traditional property valuation methods are costly and manual in nature, requiring an appraiser to assess the property and make calculations based on comparable properties. With an automated model that makes price predictions given a set of property characteristics, we have a systematic and unbiased way of getting valuation estimates which are faster and cheaper. Such a model can also reduce fraud cases in valuations, given that the property features that serve as the model's inputs are not tampered with. Appraisers can use this model to get a first-cut estimate on valuation of properties and supplement it with additional human knowledge of the uniqueness of each property. Furthermore, sellers can use the model to get an estimation of their existing property\u2019s value to arrive at a competitive ask price, while buyers can be more informed to fairly assess their options. In essence, all parties are able to come towards a conclusive price point for a transaction to happen without under- or over-valuing the property.","title":"Background"},{"location":"motivation/#main-goals","text":"With our motivation in mind, we wanted to do three things in this project:","title":"Main goals"},{"location":"motivation/#making-an-accurate-price-prediction-model","text":"First, we aim to create an ML model that can accurately make price predictions given property characteristics. Our plan was to try out different machine learning models that would be suitable for this task and dataset, and then evaluate their performance to determine the best models.","title":"Making an accurate price prediction model"},{"location":"motivation/#feature-importance-analysis","text":"We also aim to identify the features most responsible for a property's resale price, to derive interesting observations from our model's performance on the dataset. This will be informative to home buyers on what are the most influential features that will affect a HDB flat\u2019s value.","title":"Feature importance analysis"},{"location":"motivation/#identify-under-and-over-valued-properties-using-model-predictions","text":"Lastly, we also wanted to run a simulation to study how well our model would perform if we were to use its predictions to identify undervalued/overvalued properties on a rolling basis. This is to say, if someone were to use our model to purchase an identified undervalued property, would they see a good return on investment after 10 years?","title":"Identify under- and over-valued properties using model predictions"},{"location":"navigate-repo/","text":"The repository for our project is available on GitHub . data_raw : All the raw data taken from our dataset sources data_processed : Data that has undergone some form of processing docs : Files for documentation website; necessary for MkDocs to function well evaluations : In-depth model evaluations we completed for three selected models, namely a rolling bagging regressor, rolling kNN regressor, and rolling XGBoost regressor. Preliminary performance metrics are computed in each model's Jupyter Notebook instead of here (these are under models ). site : Compiled documentation website after running mkdocs build . Important note: The data files (includes files with extensions .csv , .pkl , and .sav ) that we have used and/or created are not stored in this repo due to storage constraints. This repository is exclusively meant for displaying the work we have completed, and hence we deem the Jupyter Notebooks and figures we have made to be enough proof of such.","title":"Navigating the repository"},{"location":"results-biased/","text":"Results # Metric MSE RMSE MAE \\(R^2\\) statistic MAPE (%) MdAPE (%) Linear regression 1352593.034 1163.010333 867.0930714 0.2794147959 27.86183292 16.06557412 Polynomial regression 1051763.669 1025.555298 767.3399047 0.4396796974 24.53005988 14.29984301 kNN regressor 12.19616818 3.492301272 1.578744179 0.9999934177 0.04201759327 0.02590745257 XGBoost regressor 85318.35251 292.0930545 216.6817757 0.9539896815 5.771889484 4.133103921 Neural networks (56-20-1) 90932.32792 301.5498763 219.5870593 0.9509621876 5.805839988 4.108291812 Naive decision trees 101915.8061 319.2425506 228.7668409 0.944995967 6.035996068 4.125049304 AdaBoost (with DT) 792811.0638 890.3993845 754.9641695 0.5721193057 22.78368677 17.78113201 Bagging regressor (with DT) 68022.14095 260.8105461 187.8941249 0.9632884022 5.089858455 3.417956829 Abbreviations used: # MSE: mean squared error RMSE: root mean squared error MAE: mean absolute error MAPE: mean absolute percentage error MdAPE: median absolute percentage error","title":"Appendix: Biased models' results"},{"location":"results-biased/#results","text":"Metric MSE RMSE MAE \\(R^2\\) statistic MAPE (%) MdAPE (%) Linear regression 1352593.034 1163.010333 867.0930714 0.2794147959 27.86183292 16.06557412 Polynomial regression 1051763.669 1025.555298 767.3399047 0.4396796974 24.53005988 14.29984301 kNN regressor 12.19616818 3.492301272 1.578744179 0.9999934177 0.04201759327 0.02590745257 XGBoost regressor 85318.35251 292.0930545 216.6817757 0.9539896815 5.771889484 4.133103921 Neural networks (56-20-1) 90932.32792 301.5498763 219.5870593 0.9509621876 5.805839988 4.108291812 Naive decision trees 101915.8061 319.2425506 228.7668409 0.944995967 6.035996068 4.125049304 AdaBoost (with DT) 792811.0638 890.3993845 754.9641695 0.5721193057 22.78368677 17.78113201 Bagging regressor (with DT) 68022.14095 260.8105461 187.8941249 0.9632884022 5.089858455 3.417956829","title":"Results"},{"location":"results-biased/#abbreviations-used","text":"MSE: mean squared error RMSE: root mean squared error MAE: mean absolute error MAPE: mean absolute percentage error MdAPE: median absolute percentage error","title":"Abbreviations used:"},{"location":"results-unbiased/","text":"Results # Metric MSE RMSE MAE \\(R^2\\) statistic MAPE (%) MdAPE (%) Rolling linear regression 110371.9301 332.2227116 234.0703576 0.9403432378 7.636594873 5.86419415 Rolling polynomial regression 884630.4331 940.547943 393.5678591 0.4983602673 12.2984977 5.491194888 kNN regressor 708562.9645 841.7618217 583.6696106 0.6447619117 10.37620696 8.242460991 Rolling kNN regressor 69306.98241 263.2621933 187.9282248 0.9625391151 6.412249097 4.733349022 Naive XGBoost 469450.7669 685.1647735 498.7413995 0.7646408275 8.971157778 7.471825957 Rolling XGBoost 67758.72249 260.3050566 188.7835581 0.9633759599 6.501568004 4.831666041 Neural networks (57-20-1) 2173130.493 1474.154162 1322.964328 -0.08949911367 24.5471328 25.18931498 Rolling neural networks 264933.6857 514.7170928 322.0846974 0.8566008466 10.2374128 7.382278679 Naive decision trees 589915.7445 768.0597272 550.3918629 0.7042457031 10.05114227 8.073536906 Rolling decision trees 94968.42101 308.169468 216.5232416 0.9486689369 7.211369358 5.384585412 AdaBoost (with DT) 1197958.106 1094.512725 903.7533006 0.3994036256 18.42378427 16.25435846 Rolling AdaBoost (with DT) 262510.5105 512.3577954 379.1674719 0.8581113234 12.16986269 9.953531307 Bagging regressor (with DT) 399350.6124 631.9419375 460.7829264 0.7997855445 8.243560544 6.856250353 Rolling bagging regressor (with DT) 65762.08751 256.441197 184.4860468 0.9644551544 6.295581978 4.707222345 Abbreviations used: # MSE: mean squared error RMSE: root mean squared error MAE: mean absolute error MAPE: mean absolute percentage error MdAPE: median absolute percentage error","title":"Appendix: Unbiased models' results"},{"location":"results-unbiased/#results","text":"Metric MSE RMSE MAE \\(R^2\\) statistic MAPE (%) MdAPE (%) Rolling linear regression 110371.9301 332.2227116 234.0703576 0.9403432378 7.636594873 5.86419415 Rolling polynomial regression 884630.4331 940.547943 393.5678591 0.4983602673 12.2984977 5.491194888 kNN regressor 708562.9645 841.7618217 583.6696106 0.6447619117 10.37620696 8.242460991 Rolling kNN regressor 69306.98241 263.2621933 187.9282248 0.9625391151 6.412249097 4.733349022 Naive XGBoost 469450.7669 685.1647735 498.7413995 0.7646408275 8.971157778 7.471825957 Rolling XGBoost 67758.72249 260.3050566 188.7835581 0.9633759599 6.501568004 4.831666041 Neural networks (57-20-1) 2173130.493 1474.154162 1322.964328 -0.08949911367 24.5471328 25.18931498 Rolling neural networks 264933.6857 514.7170928 322.0846974 0.8566008466 10.2374128 7.382278679 Naive decision trees 589915.7445 768.0597272 550.3918629 0.7042457031 10.05114227 8.073536906 Rolling decision trees 94968.42101 308.169468 216.5232416 0.9486689369 7.211369358 5.384585412 AdaBoost (with DT) 1197958.106 1094.512725 903.7533006 0.3994036256 18.42378427 16.25435846 Rolling AdaBoost (with DT) 262510.5105 512.3577954 379.1674719 0.8581113234 12.16986269 9.953531307 Bagging regressor (with DT) 399350.6124 631.9419375 460.7829264 0.7997855445 8.243560544 6.856250353 Rolling bagging regressor (with DT) 65762.08751 256.441197 184.4860468 0.9644551544 6.295581978 4.707222345","title":"Results"},{"location":"results-unbiased/#abbreviations-used","text":"MSE: mean squared error RMSE: root mean squared error MAE: mean absolute error MAPE: mean absolute percentage error MdAPE: median absolute percentage error","title":"Abbreviations used:"},{"location":"under-overvalued/","text":"General description # For this analysis, we aim to answer the following question: \u201cIf someone were to use our model to purchase a property identified as undervalued, would they see a good return on investment after 10 years?\u201d Simulation # Benchmark refers to the average return on investment of all the properties in the particular timeframe. In a certain time period, using our model we get the present value and the value 10 years from that time. Using these two values, we can compute the ROI. If ROI is above the red line, we will make a profit and vice versa. As the undervalued ROI is always above the benchmark ROI, our model successfully predicts which properties are undervalued, and vice versa for the overvalued. Assumptions # We are not accounting for one-off purchase costs, stamp duties, taxes, mortgage loans etc. Accounting for these would suppress some of the more optimistic ROI numbers that we see in this study. We have access to the ground truth values, but we may have to use listing data instead for real-life usage. Our model has a non-zero error rate hence there is some bias in our ROI computations.","title":"Undervalued and overvalued analysis"},{"location":"under-overvalued/#general-description","text":"For this analysis, we aim to answer the following question: \u201cIf someone were to use our model to purchase a property identified as undervalued, would they see a good return on investment after 10 years?\u201d","title":"General description"},{"location":"under-overvalued/#simulation","text":"Benchmark refers to the average return on investment of all the properties in the particular timeframe. In a certain time period, using our model we get the present value and the value 10 years from that time. Using these two values, we can compute the ROI. If ROI is above the red line, we will make a profit and vice versa. As the undervalued ROI is always above the benchmark ROI, our model successfully predicts which properties are undervalued, and vice versa for the overvalued.","title":"Simulation"},{"location":"under-overvalued/#assumptions","text":"We are not accounting for one-off purchase costs, stamp duties, taxes, mortgage loans etc. Accounting for these would suppress some of the more optimistic ROI numbers that we see in this study. We have access to the ground truth values, but we may have to use listing data instead for real-life usage. Our model has a non-zero error rate hence there is some bias in our ROI computations.","title":"Assumptions"}]}